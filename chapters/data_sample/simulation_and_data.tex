\section{Event Simulation}
\label{sec:event-simulation}
The method by which all of the measurements presented in this thesis are performed is that of \emph{Monte-Carlo (MC) forward folding}.
In a nutshell, this method involves producing a large set of simulated signal and background events that are then re-weighted in such a way that their distribution matches that of the observed data events as closely as possible.
To give reliable results, an accurate simulation of all particle interactions described in Section~\ref{sec:particle-interactions} as well as the detector electronics described in section~\ref{sec:dom-daq} is required.
The simulated and observed events are then passed through the same data processing chain described in section~\ref{sec:data-processing}.
The resulting MC simulated dataset and the observed dataset are then histogrammed in the same binning, and the weights of the MC events are adjusted to give the best match between the histograms according to a loss function as defined in section~\ref{sec:test-statistic}.

The simulation chain for neutrinos and atmospheric muons can generally be divided into three steps that are described in this chapter:
\begin{enumerate}
    \item Simulation of particle interactions
    \item Photon propagation in ice
    \item Response of detector DAQ systems
\end{enumerate}
A special case is the simulation of detector noise, for which no particle production or photon propagation is necessary.

% \subsection{Particle Interactions}

% The first step for the simulation of neutrinos and muons is to sample parameters for the primary particle, and to simulate the secondary charged particles that are produced when it interacts inside the detector. The charged components of the secondary particles are then passed on to the photon propagation step described in section~\ref{sec:photon-propagation}.

\subsection{Neutrino Interactions}

Because of the inherently low interaction rate of neutrinos, it would be impractical to simulate a constant flux of neutrinos from any particular direction, the vast majority of which would simply pass through the detector without producing any signal at all.
Instead, every simulated neutrino is forced to interact within a given volume, and the event is given a weight corresponding to the inverse of the simulated fluence,
\begin{equation}
    w = \frac{1}{F_{\mathrm{sim}}} \frac{1}{N_{\mathrm{sim}}}\;.
\end{equation}
Here, $N_{\mathrm{sim}}$ is the number of simulated events and $F_{\mathrm{sim}}$ is the number of neutrino events per area, solid angle, energy, and time in the simulation.
This weight, when multiplied with the flux of a given physics model and a live time, gives the expected number of events that this simulated event corresponds to.
The baseline neutrino flux model used in this work is that proposed by Honda~\emph{et.~al}\sidecite{Honda:2015fha} that is specifically computed for the South Pole\sidenote{Variations on this flux model and how they are propagated into the analysis are described in \refsec{flux_systs}.}.

%For this analysis, the simulated interaction volume is a cylinder centered in DeepCore, with a length and radius chosen such that all events that have a chance of producing a signal in DeepCore should be contained in it. The neutrino directions are sampled isotropically in azimuth and zenith, implying that the simulated flux per solid angle is $\phi_\Omega = \frac{1}{4\pi}$. The simulated neutrino flux is a power law with $\phi_e \propto E^{-2}$. After sampling the zenith and azimuth for an event, a random position is sampled
Under the assumption that neutrino absorption is negligible and that the material consists of isoscalar targets, the simulated fluence is given by the chosen probability density in the direction and energy, $\phi_\Omega \times \phi_E$,  the size of the interaction volume, $V$, the cross-section of the interaction, $\sigma$, and the density of the material, $\rho$, by
\begin{equation}
    F_{\mathrm{sim}}^{-1} = V \times \rho \times N_A \times 1\frac{\mathrm{mol}}{\mathrm{g}} \times \sigma \times \frac{1}{\phi_\Omega} \times \frac{1}{\phi_E}\;,
\end{equation}
where $N_A$ is Avogadro's number. The volume in which neutrino interactions are simulated is a cylinder centered in DeepCore, with a height and radius chosen such that all events that have a chance of producing a signal in DeepCore should be contained in it, depending on the neutrino flavor and energy (see also table~\ref{table:GENIE}). Neutrino directions are isotropically distributed in zenith and azimuth, implying $\phi_\Omega = \frac{1}{4\pi}$. The neutrino energies are sampled from a power law with $\phi_e \propto E^{-2}$. The simulated live time corresponding to a single simulated event is  $T_{\mathrm{sim}} =  F_{\mathrm{sim}} / \Phi$, where $\Phi$ is the expected neutrino flux including neutrino oscillations at global best-fit parameters.
The amount of simulation generated for each neutrino flavor is chosen such that the total simulated live time is $>70$~years over the entire energy range.
Neutrinos and anti-neutrinos are produced in ratios of 70\% and 30\%, respectively.
The simulated live time as a function of energy is shown in \reffig{sim-livetime}.
The livetime for electron neutrinos increases with energy because the simulated spectrum is harder than the real spectrum.
The livetime for tau neutrinos is much higher than that of other flavors because the contribution of tau neutrinos to the expected neutrino flux is very small.

\begin{table}
\caption{Table of generation volumes used for \textsc{Genie} neutrino simulation. The cylinder is centered in DeepCore in all cases. \label{table:GENIE}}
\begin{center}
\begin{tabular}{ ccccc }
\textbf{Flavor} & \textbf{Energy (GeV)} & \textbf{Radius (m)} & \textbf{Length (m)}\\
\toprule
\multirow{4}{*}{$\nu_e+\bar{\nu_e}$}  & 1-4 & 250 & 500 \\
 & 4-12 & 250 & 500   \\
 & 12-100 & 350 & 600  \\
 & 100-10000 & 550 & 1000  \\
 \midrule
\multirow{4}{*}{$\nu_{\mu}+\bar{\nu_{\mu}}$} & 1-5 & 250 & 500\\
 & 5-80 & 400 & 900\\
 & 80-1000 & 450 & 1500\\
 & 1000-10000 & 550 & 1500\\
 \midrule
\multirow{5}{*}{$\nu_{\tau}+\bar{\nu_{\tau}}$} & 1-4 & 250 & 500\\
 & 4-10 & 250 & 500\\
 & 10-50 & 350 & 600\\
 & 50-1000 & 450 & 800\\
 & 1000-10000 & 550 & 1500\\
 \bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure}
    \centering
    \input{figures/icecube/selection/livetime/livetime.tex}
    \caption{Simulated MC livetime as a function of energy, calculated using the HKKM\cite{Honda:2015fha} model flux with \textsc{NuFit}~2.2\cite{nufit22} oscillation parameters.}
    \label{fig:sim-livetime}
\end{figure}

After sampling the parameters of the primary neutrino, the \textsc{Genie}\sidecite{Andreopoulos:2015wxa} software is used to simulate its interaction with the ice and the production of secondary particles and to calculate the cross-section of the interaction.
The propagation and Cherenkov light production of any muon that is produced in these interactions is simulated with \textsc{Proposal}\sidecite{proposal}.
The light output of secondary electrons, positrons, and gamma rays above 100~MeV, and that of hadronic showers above 30~GeV, are generated using analytic approximations from \cite{RADEL2013102} as described in sections \ref{sec:em-showers} and \ref{sec:had-showers}.
At lower energies, the full \textsc{Geant4} simulation of the shower development is run to produce Cherenkov photons.

\subsubsection{Cross-section uncertainties}
\label{sec:xsec_systs}
Two systematic parameters are included to account for uncertainties in the form factors of charged-current quasi-elastic ($M_{A}^{CCQE}$) events and charged-current resonant ($M_{A}^{CCRES}$) events. Both these form factors have a dependency on the momentum transfer, $Q^2$, of the form:\\

\begin{equation}
    F(Q^{2}) \propto \frac{1}{(1-(Q^{2}/M_{A}^{2})^{2}}
\end{equation}

Where $M_{A}$ is called the \textit{axial mass}, and can be measured experimentally.
The differential cross-section of each event is computed with \textsc{GENIE} at five discrete points, that is, the nominal mass and  -2$\sigma$,-1$\sigma$,1$\sigma$ and 2$\sigma$ away from the nominal mass, where $\sigma$ is a fractional uncertainty of 20\%.
This uncertainty approximates the recommendation of the GENIE collaboration, which suggests an asymmetric error of -15\% and +25\% for $M_A^{CCQE}$ and a symmetric error of $\pm20\%$ for $M_A^{CCRES}$\cite{Andreopoulos:2015wxa}.
In order to apply a continuous variation of that systematic parameter over the course of a minimization, a quadratic function is fit to interpolate between these discrete points.
\reffig{resonant_mass} shows the \textsc{GENIE} weights of a handful of $\nu_{e}$ CC events from resonance production, across the allowed range of axial masses, along with their fitted quadratic dependence.
The upper panel of \reffig{template_xsecsyst} illustrates an example of the varying $M_{A}^{RES}$ on the final level sample.

\begin{figure}
    \centering
    \input{figures/measurement/systematics/xsec/genie_sys_res.tex}
    \caption{\textsc{GENIE} interaction weights as a function of the pull of the axial mass term $M_{A}^{\mathrm{CCRES}}$, for five $\nu_{e}$ CC events produced via resonance interactions. Each dot represents a discrete point for which the event's cross section is computed in \textsc{GENIE}. The  line represents the quadratic fit made used to interpolate the weight value over the continuous range allowed for the systematic parameter.}
    \label{fig:resonant_mass}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth,trim={0 0 0 0.6cm},clip]{figures/measurement/systematics/xsec/Genie_Ma_RES.pdf}
        \caption{GENIE $M_{A}^\mathrm{RES}$}
    \end{subfigure}
    \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth,trim={0 0 0 0.6cm},clip]{figures/measurement/systematics/xsec/dis_csms.pdf}
        \caption{DIS CSMS}
    \end{subfigure}
  \caption{Fractional difference in event rates between (top )$M_{A}^\mathrm{RES}$ (bottom) dis$\_$csms at 1$\sigma$ and at nominal value for both PID bins.
  \label{fig:template_xsecsyst}}
\end{figure}

The uncertainty on the DIS cross-section is primarily given by the disagreement in DIS calculation between CSMS\cite{csms-xsec} and GENIE\cite{Andreopoulos:2015wxa} cross-sections at energies above 100~GeV.
This analysis includes a parameter that interpolates between these two calculations with a linear extrapolation to energies below 100~GeV.
The bottom panel of \reffig{template_xsecsyst} illustrates an example of the varying this parameter, DIS, on the final level sample.
As expected, the impact of the parameter is largest in the highest energy bins.
There is an additional uncertainty of 20\% on the normalization of NC events to account for uncertainties of the hadronization process and the Weinberg angle in line with previous oscillation studies\cite{Aartsen_2018}.

\subsection{Atmospheric muons}
The offline filter steps described in section~\ref{sec:offline-filter} decrease the rate of atmospheric muons by several orders of magnitude as events pass through each of its stages.
This makes it challenging to produce a sufficiently large amount of simulated muon events to accurately estimate the expected background at the final level.
To overcome this challenge, two separate muon simulation sets are produced, one of which is used to tune the lower level (up to L4) offline filters and the other is used to estimate muon background at levels L5 and above.

For both sets, atmospheric muons are generated on the surface of a cylinder encompassing the entire IceCube detector with a radius of 800~m and a height of 1600~m.
Positions and directions of muons interacting in the detector are sampled using parametrized tables based on the approach described in \sidecite{BECHERINI20061}.
These tables are tuned to approximate the output of a detailed \textsc{CORSIKA}\sidecite{Heck1998CORSIKAAM} simulation of cosmic ray interactions and subsequent shower production using the cosmic ray flux model described in \sidecite{Gaisser:2011klf} and the \textsc{SIBYLL 2.1}\sidecite{sibyll} hadronic interaction model.
This flux is also used to weight simulated muon events and is distinct from the flux model used to weight neutrino events.
For the simulations used to tune the lower selection levels, the muon energy is sampled from a power law with a spectral index of -3 and all events are accepted to cover the entire IceCube array.
To produce the simulation that is used starting at the L5 trigger level, muons are only accepted if they intersect an inner cylinder centered in the DeepCore fiducial volume with a radius of 180~m and a height of 400~m.
Furthermore, muons are rejected based on a KDE estimate of the muon density in energy and zenith angle at the L5 filter level.
In this way, the sampling preferably produces such muon events that have a higher chance of passing the offline filtering up to L5, which greatly improves the efficiency of the simulation production.

After the position, direction and energy for a muon has been sampled, its propagation and photon production is simulated using \textsc{PROPOSAL} in just the same way as any muon that is produced in neutrino interaction would be.

\subsubsection{Muon Uncertainty}
\label{sec:atm-muons-systematic}
Because the muon background contamination is cut to only $\sim$2\% at the final level of the event selection (see \refsec{final-sample-binning}), the impact of muon systematic uncertainties is generally small.
Only the over-all scale is left as a free parameter in the analysis, its impact is shown in \reffig{weight-scale-syst}.
This scale also largely absorbs the effects of DOM efficiency uncertainties, since, to first order, an increase in DOM efficiency leads to a better muon rejection.
The spectral index of the muon flux has a very small effect far below the percent-level as shown in \reffig{delta-gamma-mu-syst} and is therefore not accounted for in the analysis.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth,trim={0 0 0 0.6cm},clip]{figures/measurement/systematics/muons/weight_scale.pdf}
    \caption{Impact on the final histograms when the muon normalization is increased by 50\%. The largest impact is seen above the horizon in the mixed PID channel with a change in bin count of 5\%.}
    \label{fig:weight-scale-syst}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth,trim={0 0 0 0.6cm},clip]{figures/measurement/systematics/muons/delta_gamma_mu.pdf}
    \caption{Impact on the final histograms when the muon spectral index is increased by $1\sigma$.}
    \label{fig:delta-gamma-mu-syst}
\end{figure}

\subsection{Photon Propagation}
\label{sec:photon-propagation}

Photons are individually traced through the ice using the GPU-accelerated \textsc{clsim}\cite{clsim} package, which is an \textsc{OpenCL} re-implementation of the Photon-Propagation Code\sidecite{ppc}.
 The ice is modeled as 10~m thick layers with individual scattering and absorption lengths that are shown in \reffig{spice-model}.
The ice model used for the simulation in this work, also referred to as \emph{South Pole ICE (SPICE)}\sidecite{flasher_calibration}, incorporates the fact that the ice layers are slightly tilted with respect to the vertical axis, and that scattering and absorption strengths are not uniform as a function of azimuth.
For every photon, \textsc{clsim} first samples the absorption length from an exponential distribution whose expectation value is the absorption length of the current layer.
It then propagates all photons in parallel steps, where every step corresponds to one scattering event and the step length is sampled from an exponential distribution where the expectation value is the scattering length of the current layer.
The scattering angle is then sampled from a mixture of a Henyey-Greenstein distribution and a simplified Mie scattering distribution, where the shape parameters of these distributions have previously been calibrated using the in-situ LED calibration system\cite{flasher_calibration}.
Each photon stops when it has either reached its total absorption length or if it has intersected a DOM.
After all photons have either been absorbed or reached a sensor, the simulations stops and passes the photons that reached a sensor on to the next step simulating the detector response.

\begin{figure}
    \centering
    %\includegraphics[width=0.9\linewidth]{figures/icecube/ice/Spice3.2.1_layered_scatt_abs_withlength_annotated.png}
    \input{figures/icecube/ice/spice_model/spice_model.tex}
    \caption{Scattering and absorption lengths as a function of depth in the South Pole Ice (SPICE) model that is used to produce the simulation for this work.}
    \label{fig:spice-model}
\end{figure}


\subsection{Simulation of Detector Response}
\label{sec:sim-detector-response}
After the photons have reached the surface of the optical sensors, the simulation determines for each one if it is converted into a Monte-Carlo photo-electron (MCPE).
The probability that this occurs depends on the wavelength-dependent sensitivity of the DOM, as well as the angular acceptance.
The angular acceptance not only depends on the geometry of the DOM itself, but also incorporates the effect of the re-frozen column of ice at the center of each bore hole.
If a photon is accepted and converted into an MCPE, the next step is to simulate how much charge would be measured by the PMT inside the DOM as a response.
The charge is drawn from a combination of a normal distribution and two exponential distributions whose parameters have been calibrated \emph{in-situ} to match the observed charge distribution in each individual DOM\sidecite{ic_spe_20}.
This distribution, also referred to as the Single Photo-Electron (SPE) template, is shown in \reffig{spe-templates}.
The MCPEs with the samples charge are then converted into simulated waveforms for the ATWD and fADC readouts which are then passed into the data processing chain starting from the \emph{wavedeform} algorithm described in Section~\ref{sec:dom-daq}.
From there, the simulated events pass through all the same trigger and filter steps that are described in Section~\ref{sec:data-processing}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/icecube/detector_response/SPE_TA003_2.pdf}
    \caption{The green (yellow) regions show the 68\% (90\%) spread in the SPE charge templates for a given charge.  Superimposed are the average SPE charge templates for the variety of hardware configurations shown in the black dotted, dashed, and solid lines. The TA0003 distribution, shown in red, originates from laboratory measurements. Figure taken from \cite{ic_spe_20}.}
    \label{fig:spe-templates}
\end{figure}

\subsubsection{Detector Noise}

\begin{margintable}
\caption{\label{tab:vuvuzela_params} Parameters used in the noise simulation. Typical values taken from \cite{Michael_Larson_masters}, actual values are fit for each DOM individually.}
    \begin{tabular}{lc}\toprule
        \textbf{Parameter} & \textbf{Typical value} \\ \midrule
        Therm. rate &  $\lambda_\mathrm{th}\approx \SI{20}{\hertz}$ \\
        Decay rate &  $\lambda_\mathrm{dec}\approx \SI{250}{\hertz}$ \\
        Decay hits &  $\eta\approx 8$ \\
        Decay $\mu$ &  $\log_{10}(\frac{\mu}{\si{\nano\second}}) \approx -6$\\
        Decay $\sigma$ &  $\log_{10}(\frac{\sigma}{\si{\nano\second}}) \approx 2.7$ \\ \bottomrule
    \end{tabular}
\end{margintable}
In addition to Cherenkov photons induced by relativistic charged particles in the ice, IceCube detects photons from radioactive decays inside the glass housing of the DOMs and PMTs that are simulated using the \emph{Vuvuzela} module\sidecite{Michael_Larson_masters}\sidecite{Michael_Larson_phd}.
These "noise" MCPEs are simulated parametrically by sampling their times from distributions that take both thermal and non-thermal noise components into account. The thermal component comes from uncorrelated photons and PMT dark noise and is modeled as a Poisson process with a constant rate.
The non-thermal component comes from correlated bursts of photons that are produced by radioactive decays.
To simulate it, decay times are first drawn from a Poisson process with a constant rate, and the number of photons produced in each decay is sampled from a Poisson distribution.
The time differences between the non-thermal MCPEs produced by each decay are then sampled from a Log-Gaussian distribution.
This simulation method has five free parameters listed in \reftab{vuvuzela_params} that are calibrated \emph{in-situ} for every DOM.
All thermal and non-thermal MCPEs are injected into each simulated event together with the MCPEs from photons and passed into the rest of the simulation chain.
% \begin{margintable}
% \caption{\label{tab:vuvuzela_params} Parameters used in the noise simulation. }
%     \begin{tabular}{lcc}\toprule
%         \textbf{Parameter} & \textbf{Designation} & \textbf{Unit} \\ \midrule
%         Thermal rate & $\lambda_{Th}$ & $s^{-1}$ \\
%         Decay rate & $\lambda_{Decay}$ & $s^{-1}$ \\
%         Scintillation hits & $\eta_{Scint}$ & hits \\
%         Scintillation mean & $\mu_{Scint}$ & $\log_{10} (ns) $\\
%         Scintillation sigma & $\sigma_{Scint}$ &  $\log_{10} (ns) $ \\ \bottomrule
%     \end{tabular}
% \end{margintable}



